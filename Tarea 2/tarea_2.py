# -*- coding: utf-8 -*-
"""Tarea_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Le8FNm-3M8auE_cx9qIb5saxu-5yvx9R

EAE3709 APLICACIONES DE  MACHINE LEARNING EN ECONOMÍA <br>
1ER SEMESTRE 2025 <br>
INSTITUTO DE ECONOMÍA <br>
PONTIFICIA UNIVERSIDAD CATÓLICA DE CHILE


# **TAREA 2**


---


Profesor:
- Joaquín Pérez Lapillo

Ayudantes:

- Luis García B.
- Sebastián Hernández B.
- Oscar Herrera G.

**Complete sus datos:**

- Nombre y apellido:
  - `# Xiomara Kuwae`
  - `# Luis José López`
- Usuario de GitHub (opcional):  `# https://github.com/luijolo/Machine-Learning/tree/main/Tarea%202`

## Instrucciones

- Descargue el notebook y cárguelo en su Drive.
- Todas las preguntas deben ser contestadas en su notebook.
- Para que una pregunta esté correcta el código debe correr.
- Si es necesario, realice comentarios breves en su código explicando lo que está realizando o sus resultados.
- Una vez culminada su tarea, suba su notebook al buzón de tareas de Canvas.
- La fecha y hora límite de esta tarea es el _**viernes 30 de mayo a las 20:00 hrs**_.

### Distribución de puntaje


| Pregunta                        | Puntaje |
|--------------------------------|---------|
| Pregunta 1.0                   |    1    |
| Pregunta 1.1                   |    2    |
| Pregunta 1.2                   |    2    |
| Pregunta 1.3                   |    2    |
| Pregunta 1.4                   |    5    |
| Pregunta 1.5                   |    5    |
| Pregunta 1.6                   |    2    |
| Pregunta 1.7                   |    2    |
| Pregunta 1.8                   |    3    |
| Pregunta 1.9                   |    3    |
| Pregunta 1.10                   |    5    |
| Pregunta 1.11                   |    5    |
| Pregunta 1.12                   |    5    |
| Pregunta 1.13                   |    3    |
| Pregunta 1.13                   |    5    |
| Pregunta 2.0                   |    1    |
| Pregunta 2.1                   |    1    |
| Pregunta 2.2                   |    1    |
| Pregunta 2.3                   |    3    |
| Pregunta 2.4                   |    5    |
| Pregunta 2.5                   |    3    |
| Pregunta 2.6                   |    3    |
| Pregunta 2.7                   |    3    |
| Pregunta 2.8                   |    5    |
| Pregunta 2.9                   |    7    |
| Pregunta 2.10                   |    4    |
| Pregunta 2.11                   |    3    |
| Pregunta 2.12                   |    1    |
| Pregunta 2.13                   |    2    |
| Pregunta 2.14                   |    3    |
| Pregunta 2.15                   |    2    |
| Pregunta 2.16                   |    3    |

Total: 100 pts.

# 1. Clasificación

En esta pregunta trabajaremos con un _dataset_ que contiene información campañas de marketing de un banco portugués. La campaña se enfocó en llamadas telefónicas a distintos segmentos de clientes, de forma individualizada, con el objetivo incenitvar la toma depósitos por parte de los clientes.

El problema presentado en este ejecicio busca predecir si el cliente suscribirá (sí/no) un depósito a plazo.



A continuación, se despliega información oficial sobre el _dataset_:

| Variable Name | Role    | Type        | Description                                                                                                                               |
|---------------|---------|-------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| age           | Feature | Integer     |     years of age                                                                                                                                      |
| job           | Feature | Categorical | type of job |
| marital       | Feature | Categorical | marital status                          |
| education     | Feature | Categorical | Education level         |
| default       | Feature | Binary      | has credit in default?                                                                                                                    |
| balance       | Feature | Integer     | average yearly balance in euros                                                                                                                    |
| housing       | Feature | Binary      | has housing loan?                                                                                                                         |
| loan          | Feature | Binary      | has personal loan?                                                                                                                        |
| contact       | Feature | Categorical | contact communication type                                                                         |
| day_of_week   | Feature | Date        | last contact day of the month (numeric)                                                                                                              |
| month         | Feature | Date        | last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')                                                          |
| duration*      | Feature | Integer     | last contact duration, in seconds (numeric).  |
| campaign      | Feature | Integer     | number of contacts performed during this campaign and for this client                                      |
| pdays         | Feature | Integer     | number of days that passed by after the client was last contacted from a previous campaign (numeric; -1 means client was not previously contacted) |
| previous      | Feature | Integer     | number of contacts performed before this campaign and for this client                                                                       |
| poutcome      | Feature | Categorical | outcome of the previous marketing campaign                                                |
| y             | Target  | Binary      | has the client subscribed a term deposit?                                                                                                 |


*Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.

En el sitio web [https://archive.ics.uci.edu/dataset/222/bank+marketing](https://archive.ics.uci.edu/dataset/222/bank+marketing) podrá encontrar información oficial del _dataset_.

Use el código otorgado a continuación para importar los datos.
"""
!pip install ucimlrepo

from ucimlrepo import fetch_ucirepo

# fetch dataset
bank_marketing = fetch_ucirepo(id=222)

# data (as pandas dataframes)
X = bank_marketing.data.features
y = bank_marketing.data.targets

"""## Exploratory Data Analysis (EDA)

### Pregunta 1.0

Concatene `X` e `y` en un dataframe consolidado.

Muestre las primeras cinco observaciones en el dataframe consolidado. Despliegue información sobre el tipo de variable de cada una de las columnas del dataframe y despliegue estadísticas descriptivas de las variables según su tipo.
"""

import pandas as pd

# 1. Concatenar X e y en un DataFrame consolidado
# Usamos pd.concat. Asumimos que quieres concatenar horizontalmente (columnas)
df_consolidado = pd.concat([X, y], axis=1)

# Se elimina la variable "duration"
df_consolidado = df_consolidado.drop('duration', axis=1)

# 2. Mostrar las primeras cinco observaciones
print("Primeras cinco observaciones del DataFrame consolidado:")
print(df_consolidado.head())

# 3. Mostrar información sobre el tipo de variable de cada columna
print("\nInformación sobre los tipos de datos de las columnas:")
print(df_consolidado.dtypes)

# 4. Mostrar estadísticas descriptivas según el tipo de variable
# Estadísticas para variables numéricas
print("\nEstadísticas descriptivas para variables numéricas:")
print(df_consolidado.select_dtypes(include=['int64', 'float64']).describe())

# Estadísticas para variables categóricas (si las hay)
print("\nEstadísticas descriptivas para variables categóricas:")
print(df_consolidado.select_dtypes(include=['object', 'category']).describe())

df_consolidado.head()


"""### Pregunta 1.1

Identifique las columnas que presentan _missing values_ e indique el número de _missing values_ que poseen.

Impute los valores nulos con el método que estime conveniente, justificando su decisión.
"""

# 1. Identificar columnas con missing values y contarlos
print("Columnas con valores nulos y su cantidad:")
missing_values = df_consolidado.isnull().sum()
missing_values = missing_values[missing_values > 0]  # Filtrar solo columnas con nulos
print(missing_values)

# 2. Se elimina la variable "poutcome", por el gran % de missing (82%)
df_consolidado = df_consolidado.drop('poutcome', axis=1)

# 3. Actualizar la lista de columnas con valores nulos después de eliminar 'poutcome'
missing_values = df_consolidado.isnull().sum()
missing_values = missing_values[missing_values > 0]  # Filtrar solo columnas con nulos

# 4. Imputar valores nulos para variables categóricas
for column in missing_values.index:
    # Imputar con la moda para variables categóricas
    df_consolidado[column] = df_consolidado[column].fillna(df_consolidado[column].mode()[0])
    print(f"Imputado '{column}' (categórica) con la moda: {df_consolidado[column].mode()[0]}")

# 5. Verificar que no queden valores nulos
print("\nValores nulos después de la imputación:")
print(df_consolidado.isnull().sum())

"""---


*Para el caso de poutcome se optó crear una categoría de unknowm para mantener la columna 
*Para contact se optó por reemplazar por la moda porque generaría el menor impacto sobre la variable que tiene solo 2 categorías.
*


---

### Pregunta 1.2


Genere histogramas de las variables categóricas desagregando por la variable objetivo de interés. Interprete las relaciones que observa.

Responda: ¿Cuáles creen que son las variables categóricas más relevantes a la hora de determinar si el individuo hará un depósito?
"""

import matplotlib.pyplot as plt
import seaborn as sns

# Lista de variables categóricas
categorical_vars = ['job', 'marital', 'education', 'default', 'housing', 'loan', 'contact', 'month']

# Configurar el estilo de los gráficos
sns.set(style="whitegrid")

# Crear una figura con subplots en una cuadrícula 2x4
fig, axes = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))

# Aplanar el array de ejes para facilitar la iteración
axes = axes.flatten()

# Generar histogramas para cada variable categórica
for i, var in enumerate(categorical_vars):
    sns.countplot(data=df_consolidado, x=var, hue='y', palette='Set2', ax=axes[i])
    axes[i].set_title(f'Distribución de {var} por Suscripción')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Frecuencia')
    axes[i].tick_params(axis='x', rotation=45)
    axes[i].legend(title='Suscripción (y)', labels=['No', 'Sí'])

# Ajustar el diseño para evitar solapamiento
plt.tight_layout()

# Mostrar la figura
plt.show()

"""---

Al analizar los histogramas, la variable “month” muestra variaciones significativas, con algunos meses (marzo, septiembre y octubre) presentando mayores tasas de “Sí”, sugiriendo un efecto estacional en la efectividad de las campañas, por lo que es una variable con potencial predictivo. Por otra parte, destaca “job” por las proporciones de “Sí” en “retired” y “student” frente a “blue-collar” y “management”, reflejando diferencias socioeconómicas. 

Asimismo, destaca la variable “education”, la que indica que “tertiary” tiene una mayor predisposición a tomar el depósito, mientras que las variables “housing” y “loan” sugieren que la ausencia de préstamos favorece la suscripción”.

Por último, variables como “marital”, “contact” y “default” parecen tener menos potencial predictivo, ya que muestran efectos menos marcados, con menor impacto predictivo.

---

### Pregunta 1.3

Identifique _outliers_ entre las variables numéricas del dataset. Además, impute estas observaciones si usted lo considera necesario. Justifique su decisión.
"""
import seaborn as sns
import matplotlib.pyplot as plt

# Lista de variables numéricas
numericas = ['age', 'balance', 'campaign', 'pdays', 'previous']

# Vemos primero las distribuciones de las variables
fig, axes = plt.subplots(2, 3, figsize=(15, 8))  # 2 rows, 3 columns
axes = axes.flatten()

for i, var in enumerate(numericas):
    axes[i].hist(df_consolidado[var].dropna(), bins=30, edgecolor='black')
    axes[i].set_title(f'Distribución de {var}')
    axes[i].set_xlabel(var)
    axes[i].set_ylabel('Frecuencia')

# Ocultar el subplot vacío (sexto subplot)
axes[5].set_visible(False)

plt.tight_layout()
plt.show()

# Boxplot para ver outliers
fig, axes = plt.subplots(2, 3, figsize=(15, 8))  # 2 rows, 3 columns
axes = axes.flatten()

for i, var in enumerate(numericas):
    sns.boxplot(y=df_consolidado[var], ax=axes[i])
    axes[i].set_title(var)

# Ocultar el subplot vacío (sexto subplot)
axes[5].set_visible(False)

plt.tight_layout()
plt.show()

# Verificar outliers en 'previous'
print(f"Contacto previo > 100: {len(df_consolidado[df_consolidado['previous'] > 100])}")

# Filtrar outliers en 'previous'
df_consolidado = df_consolidado[df_consolidado['previous'] <= 99]


"""---


* Se eliminaron las personas contactadas 100 veces o más previamente (1). 


---

## Feature Engineering

### Pregunta 1.4

 Cree un _pipeline_ de preprocesamiento con las siguientes transformaciones a las columnas correspondientes:

*   **Columnas Numéricas** : Aplique un `StandardScaler` para estandarizar estas variables.
*   **Columnas Categóricas No Binarias** : Aplique `OneHotEncoder` para crear variables dummy. Asegúrese de manejar posibles categorías desconocidas durante la transformación.
*   **Columnas a Eliminar** : Elimine columnas que dejan de ser relavantes luego de las trasformaciones.
*  **Columnas Dummies MultiNivel** : cree columnas dummies para las variables multi categoricas (tomando codificadas a 0s y 1s =, para su respctiva categoria)
*  **Columnas Label Encoding MultiNivel** : Dentro de las columnas multinivel hay 2 columnas que presentan un orden natural. e.g. en la columna 'month' tenemos que 'jan' < 'feb' < 'dec'. Aplique en la columna 'month' y otra columna que deberá determinar usted un label encoder que capture ese ordenamiento natural, luego normalice con un minmax scaler para que las columnas varien entre 0 y 1.


Una vez creado el pipeline, aplíquelo a su conjunto de features (`X`). Muestre la forma (_shape_) del dataset original y del dataset procesado para verificar los cambios. ¿Cuántas nuevas columnas hay?

Adicionalmente, elimine del análisis la variable `day_of_week`. Si considera necesario realizar transformaciones adicionales a las variables, realicelas en este punto.
"""

import numpy as np
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler, LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.base import BaseEstimator, TransformerMixin

# Transformador personalizado para aplicar LabelEncoder a múltiples columnas
class MultiColumnLabelEncoder(BaseEstimator, TransformerMixin):
    def __init__(self):
        self.encoders = {}

    def fit(self, X, y=None):
        for column in X.columns:
            le = LabelEncoder()
            le.fit(X[column])
            self.encoders[column] = le
        return self

    def transform(self, X):
        X_transformed = X.copy()
        for column, le in self.encoders.items():
            X_transformed[column] = le.transform(X[column])
        return X_transformed

# Definir columnas
numericas = ['age', 'balance', 'campaign', 'pdays', 'previous']
categoricas_binarias = ['default', 'housing', 'loan']
dummies_multinivel = ['job', 'marital', 'contact']
label_ordinal = ['month', 'education']  # month y education con orden natural

# Crear DataFrame de características (X) excluyendo 'y'
X = df_consolidado.drop(columns=['y'])
y = df_consolidado['y']

# Transformar y (de "yes"/"no" a 0/1)
y_encoder = LabelEncoder()
y_encoded = y_encoder.fit_transform(y)  # "no" -> 0, "yes" -> 1
print("Primeros valores de y_encoded:", y_encoded[:5])

# Definir transformaciones para X
preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numericas),
        ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), categoricas_binarias),
        ('dummies', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), dummies_multinivel),
        ('ordinal', Pipeline([
            ('label', MultiColumnLabelEncoder()),
            ('scale', MinMaxScaler())
        ]), label_ordinal)
    ],
    remainder='drop'  # Eliminar columnas no transformadas (e.g., day_of_week)
)

# Crear y aplicar el pipeline a X
pipeline = Pipeline([
    ('preprocessor', preprocessor)
])

# Transformar X
X_processed = pipeline.fit_transform(X)

# Obtener nombres de las nuevas columnas (para inspección)
num_cols = numericas
cat_transformer = pipeline.named_steps['preprocessor'].named_transformers_['cat']
cat_cols = cat_transformer.get_feature_names_out(categoricas_binarias)
dum_transformer = pipeline.named_steps['preprocessor'].named_transformers_['dummies']
dum_cols = dum_transformer.get_feature_names_out(dummies_multinivel)
ord_cols = label_ordinal
new_columns = list(num_cols) + list(cat_cols) + list(dum_cols) + list(ord_cols)

# Convertir X_processed a DataFrame (para inspección)
X_processed_df = pd.DataFrame(X_processed, columns=new_columns)
print("Primeras filas de X_processed_df:")
print(X_processed_df.head())
print("\nColumnas transformadas:", X_processed_df.columns.tolist())

# Verificar formas
print("\nForma del dataset original:", X.shape)
print("Forma del dataset procesado:", X_processed.shape)

# Calcular número de nuevas columnas
original_cols = len(X.columns)
processed_cols = X_processed.shape[1]
new_cols = processed_cols - original_cols
print(f"Número de nuevas columnas: {new_cols}")

=======

"""## Modelos

En esta sección aplicaremos los algoritmos de clasificación vistos en el curso. Específicamente, compararemos el rendimiento de los siguientes modelos:

*   _Decision Tree Classifier_
*   _Random Forest Classifier_
*   _XGBoost Classifier_
*   _Naïve Bayes_

### Pregunta 1.5

Divida el conjunto de datos en conjuntos de entrenamiento y de prueba. Analice la distribución de la variable objetivo en ambos conjuntos para determinar si el conjunto de datos está balanceado o no.

En caso de que el conjunto de datos no esté balanceado, asegúrese de que la división de _train_-_test_ respete el balance de las clases target para evitar que se genere un sesgo. Para esto, se recomienda leer la documentación oficial de la función `train_test_split()`: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html.

Use la semilla `random_state` = 123.
"""
from sklearn.model_selection import train_test_split

# Dividir en conjuntos de entrenamiento y prueba, respetando el balance de clases
X_train, X_test, y_train, y_test = train_test_split(X_processed, y_encoded, test_size=0.2, random_state=123, stratify=y_encoded)

# Analizar la distribución de la variable objetivo en ambos conjuntos
print("Distribución en el conjunto de entrenamiento (y_train):")
print(pd.Series(y_train).value_counts(normalize=True))
print("\nDistribución en el conjunto de prueba (y_test):")
print(pd.Series(y_test).value_counts(normalize=True))

# Verificar las formas de los conjuntos
print("\nForma de X_train:", X_train.shape)
print("Forma de X_test:", X_test.shape)

"""### Pregunta 1.6

Entrene un árbol clasificador sin definir profundad máxima del árbol. Llamaremos a este, "modelo no regularizado".

Asegúrese de que el modelo sea capaz de lidiar con potenciales desbalances. Para esto se recomienda leer la documentación oficial de la *clase* `DecisionTreeClassifier()` (parámetros de inicialización): https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html.


Despliege el $F_1\ Score$ del modelo sobre la base de entrenamiento. Luego, grafique una representación del árbol entrenado. ¿Qué puede decir sobre este modelo no regularizado?
"""
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import f1_score, precision_score, recall_score, roc_auc_score
from sklearn.model_selection import train_test_split
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

# Crear árbol de decisión sin profundidad máxima, con manejo de desbalance
dt_classifier = DecisionTreeClassifier(class_weight='balanced', random_state=123)

# Entrenar con el set de entrenamiento
dt_classifier.fit(X_train, y_train)

# Predicción sobre el conjunto de prueba
y_test_pred_dt1 = dt_classifier.predict(X_test)

# Calcular el F1 Score en el conjunto de prueba
f1_test = f1_score(y_test, y_test_pred_dt1)
print(f"F1 Score en el conjunto de prueba: {f1_test:.4f}")

# Visualizar el árbol usando plot_tree
plt.figure(figsize=(20, 10))
plot_tree(
    dt_classifier,
    feature_names=new_columns,
    class_names=['No', 'Sí'],
    filled=True,
    rounded=True,
    max_depth=3,  # Limitar la profundidad para que sea legible
    fontsize=10
)
plt.title("Árbol de Decisión (Profundidad Máxima Visualizada: 3)", fontsize=14)
plt.show()

# Lista para almacenar los resultados
results = []

# Resultados Árbol de Decisión sin profundidad máxima
results.append({
    'Modelo': 'Árbol de Decisión sin profundidad máxima',
    'F1 Score': f1_score(y_test, y_test_pred_dt1),
    'Precisión': precision_score(y_test, y_test_pred_dt1),
    'Recall': recall_score(y_test, y_test_pred_dt1),
    'AUC-ROC': roc_auc_score(y_test, dt_classifier.predict_proba(X_test)[:, 1])
})

"""---


*Como no se impuso una profundidad máxima el modelo se amplio generando un árbol demasiado largo y complejo, poco interpretable. Pese a ser un árbol tan largo y complejo, el F1 score es bastante bajo (0.25). Además tiene una mala relación tiempo de cómputo-precisión por su complejidad y tamaño. La complejidad de este modelo sugiere que se está incurriendo en overfitting: capturando ruido y detalles específicos que no generalizan bien al conjunto de prueba.


---

### Pegunta 1.7

En el contexto particular de una campaña de marketing, donde probablemente la mayoría de clientes no harán depósitos, es importante identificar correctamente tanto los clientes que harán depósitos (clase minoritaria), como los clientes que no harán depósitos.

En el primer caso, falsos negativos implican un cliente perdido. En el segundo caso, falsos positivos implicarían un gasto de marketing innecesario en clientes que no son de interés. En este sentido, si clasificamos a todos los clientes como individuos que no harán depósitos, el _accuracy_ será muy alto, pero el costo económico será muy grande.

Para penalizar tanto falsos positivos, como falsos negativos, a la vez que se busca maximizar los verdaderos positivos, nos enfocaremos en la métrica de _performance_ $F_1\ Score$.

Calcule el $F_1\ Score$ de su árbol no regularizado con la muestra correcta. ¿Considera que es un buen valor? Comente.
"""
# Predicción sobre base de testeo
y_pred = dt_classifier.predict(X_test)

# Calculamos F1 Score
f1 = f1_score(y_test, y_pred, average='binary')
print(f"F1 score: {f1:.4f}")


"""---


*Ahora al utilizar el F1 Score medido con el test de prueba, vemos que el modelo lo hace considerablemente peor, un F1 Score de 0,25 resulta bajo y demuestra que modelo no es bueno, ya que no está capturando patrones relevantes en los datos.


---

### Pregunta 1.8

Entrene un árbol clasificador con profundad máxima del árbol igual a 16. Llamaremos a este, "modelo levemente regularizado". Asegúrese de que el modelo sea capaz de lidiar con potenciales desbalances. Luego, despliege el $F_1\ Score$ del modelo sobre la base de entrenamiento.

Grafique una representación del árbol entrenado. ¿Qué puede decir sobre este modelo no regularizado?
"""

# Crear el árbol de decisión con profundidad máxima 16 y manejo de desbalance
dt_classifier = DecisionTreeClassifier(max_depth=16, class_weight='balanced', random_state=123)

# Entrenamos con set de entrenamiento
dt_classifier.fit(X_train, y_train)

# Predicción sobre el conjunto de prueba
y_test_pred_dt2 = dt_classifier.predict(X_test)

# Calcular el F1 Score en el conjunto de prueba
f1_test = f1_score(y_test, y_test_pred_dt2)
print(f"F1 Score en el conjunto de prueba: {f1_test:.4f}")

# Visualizar el árbol usando plot_tree
plt.figure(figsize=(20, 10))
plot_tree(
    dt_classifier,
    feature_names=new_columns,  # Nombres de las columnas transformadas (de la Pregunta 1.4)
    class_names=['No', 'Sí'],  # Clases de la variable objetivo
    filled=True,  # Colorear nodos según la clase mayoritaria
    rounded=True,  # Bordes redondeados para mejor visualización
    max_depth=3,  # Limitar la profundidad para que sea legible
    fontsize=10
)
plt.title("Árbol de Decisión (Profundidad Máxima Visualizada: 3)", fontsize=14)
plt.show()

# Resultados Árbol de Decisión con profundidad máxima 16
results.append({
    'Modelo': 'Árbol de Decisión sin profundidad máxima 16',
    'F1 Score': f1_score(y_test, y_test_pred_dt2),
    'Precisión': precision_score(y_test, y_test_pred_dt2),
    'Recall': recall_score(y_test, y_test_pred_dt2),
    'AUC-ROC': roc_auc_score(y_test, dt_classifier.predict_proba(X_test)[:, 1])
})

"""---


Con una profundidad máxima de 16, este modelo introduce una regularización leve, lo que reduce el sobreajuste en comparación con el modelo sin límite. Sin embargo, el F1 Score (0.3058) sigue siendo bajo. Este modelo parece estar en un punto intermedio, evitando un sobreajuste extremo, pero aún no está lo suficientemente regularizado para optimizar el F1 Score.

---

### Pregunta 1.9

Entrene un árbol clasificador con profundad máxima del árbol igual a 8. Llamaremos a este, "modelo regularizado". Asegúrese de que el modelo sea capaz de lidiar con potenciales desbalances. Luego, despliege el $F_1\ Score$ del modelo sobre la base de entrenamiento.

Grafique una representación del árbol entrenado. ¿Qué puede decir sobre este modelo no regularizado?
"""

# Crear el árbol de decisión con profundidad máxima 8 y manejo de desbalance
dt_classifier = DecisionTreeClassifier(max_depth=8, class_weight='balanced', random_state=123)

# Entrenamos con set de entrenamiento
dt_classifier.fit(X_train, y_train)

# Predicción sobre el conjunto de prueba
y_test_pred_dt3 = dt_classifier.predict(X_test)

# Calcular el F1 Score en el conjunto de prueba
f1_test = f1_score(y_test, y_test_pred_dt3)
print(f"F1 Score en el conjunto de prueba: {f1_test:.4f}")

# Visualizar el árbol usando plot_tree
plt.figure(figsize=(20, 10))
plot_tree(
    dt_classifier,
    feature_names=new_columns,  # Nombres de las columnas transformadas (de la Pregunta 1.4)
    class_names=['No', 'Sí'],  # Clases de la variable objetivo
    filled=True,  # Colorear nodos según la clase mayoritaria
    rounded=True,  # Bordes redondeados para mejor visualización
    max_depth=3,  # Limitar la profundidad para que sea legible
    fontsize=10
)
plt.title("Árbol de Decisión (Profundidad Máxima Visualizada: 3)", fontsize=14)
plt.show()

# Resultados Árbol de Decisión con profundidad máxima 8
results.append({
    'Modelo': 'Árbol de Decisión sin profundidad máxima 8',
    'F1 Score': f1_score(y_test, y_test_pred_dt3),
    'Precisión': precision_score(y_test, y_test_pred_dt3),
    'Recall': recall_score(y_test, y_test_pred_dt3),
    'AUC-ROC': roc_auc_score(y_test, dt_classifier.predict_proba(X_test)[:, 1])
})

"""---

Con una profundidad máxima de 8, este modelo aplica una regularización más fuerte, lo que previene el sobreajuste y mejora la generalización al conjunto de prueba. El F1 Score (0.3678) es el más alto entre los Árboles de Decisión.


---

### Pregunta 1.10

Entrene 20 árboles de clasificación. Cada uno de ellos debe estar entrenado fijando la profundidad máxima del árbol `max_depth` igual a $n \in [1,20]$. Obtenga el $F_1\ Score$ de cada árbol con la base de entrenamiento y la base de prueba.

Gafique en una misma figura $F_1\ Score$ de entrenamiento contra $n$ y $F_1\ Score$ de prueba contra $n$ (Eje X = $n$). ¿Cuál parece ser el parámetro óptimo $n^*$ para el árbol? ¿Qué puede decir sobre el parámetro sobre _overfitting_ y generalización?
"""

# Listas para almacenar los F1 Scores
f1_train_scores = []
f1_test_scores = []
depths = list(range(1, 21))  # Profundidades de 1 a 20

# Entrenar 20 árboles con max_depth de 1 a 20
for n in depths:
    # Crear y entrenar el árbol
    dt_classifier = DecisionTreeClassifier(max_depth=n, class_weight='balanced', random_state=123)
    dt_classifier.fit(X_train, y_train)

    # Calcular F1 Score en entrenamiento
    y_train_pred = dt_classifier.predict(X_train)
    f1_train = f1_score(y_train, y_train_pred)
    f1_train_scores.append(f1_train)

    # Calcular F1 Score en prueba
    y_test_pred = dt_classifier.predict(X_test)
    f1_test = f1_score(y_test, y_test_pred)
    f1_test_scores.append(f1_test)

# Imprimir resultados
print("F1 Scores para cada profundidad (max_depth):")
for n, f1_train, f1_test in zip(depths, f1_train_scores, f1_test_scores):
    print(f"max_depth={n}: F1 Train={f1_train:.4f}, F1 Test={f1_test:.4f}")

# Graficar F1 Scores (entrenamiento y prueba) contra max_depth
plt.figure(figsize=(10, 6))
plt.plot(depths, f1_train_scores, label='F1 Score (Entrenamiento)', marker='o', color='blue')
plt.plot(depths, f1_test_scores, label='F1 Score (Prueba)', marker='o', color='red')
plt.xlabel('Profundidad Máxima (max_depth)')
plt.ylabel('F1 Score')
plt.title('F1 Score vs. Profundidad Máxima del Árbol')
plt.legend()
plt.grid(True)
plt.xticks(depths)
plt.show()

"""---


Si para establecer cuál es el parámetro óptimo (n*) se utiliza el criterio de observar el valor que maximiza el F1 Score del set de testeo, se elige 7 como el número óptimo de profundidad máxima para el árbol de decisión.

Respecto al overfitting y generalización, en el gráfico podemos observar cómo el modelo comienza a sobreajustar a partir de la profundidad máxima de 7, mostrándose una creciente brecha entre el F1 score del set de entranemiento y el de testeo, mostrándose un claro sobreajuste en n=20 (F1 Train=0.6534, F1 Test=0.2688). En cuanto a la generalización, una profundidad intermedia, de 7, ofrece un mejor equilibrio entre aprender patrones relevantes en los datos y al mismo tiempo evitar sobreajuste. De esta manera, maximizando el desempeño en el set de testeo.


---

### Pregunta 1.11

En esta pregunta deberá entrenar _Random orest Classifier_ realizando un ejercicio previo de _hyperparameter tuning_.

Realice _Cross Validation_ con un _folding_ con `K=5` y calcule $F_1\ Score$ de todas las posibles combinaciones de los siguientes parámetros:

- `n_estimators = {100, 1000}`
- `max_depth = {10, 50, 100}`
- `min_samples_leaf = {2, 4}`

Determine la mejor combinación de parámetros y vuelva a entrenar su bosque con esta. Finalmente, reporte el $F_1\ Score$ con la muestra de prueba.

Cuando realice _Cross Validation_ y cuando entrene el modelo final, asegúrese de que el modelo sea capaz de lidiar con potenciales desbalances. Para esto se recomienda leer la documentación oficial de la *clase* `RandomForestClassifier()` (parámetros de inicialización): https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html.
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score

# Definir el espacio de hiperparámetros
param_grid = {
    'n_estimators': [100, 1000],
    'max_depth': [10, 50, 100],
    'min_samples_leaf': [2, 4]
}

# Inicializar listas para almacenar resultados
cv1_results = []
best_f1_cv = -1
best_params = None

# Realizar Cross Validation con K=5 para cada combinación
for n_est in param_grid['n_estimators']:
    for depth in param_grid['max_depth']:
        for min_leaf in param_grid['min_samples_leaf']:
            # Crear y configurar el modelo con manejo de desbalance
            rf_classifier = RandomForestClassifier(
                n_estimators=n_est,
                max_depth=depth,
                min_samples_leaf=min_leaf,
                class_weight='balanced',
                random_state=123
            )

            # Calcular F1 Score promedio con validación cruzada (K=5)
            f1_cv_scores = cross_val_score(
                rf_classifier, X_train, y_train, cv=5, scoring='f1'
            )
            mean_f1_cv = f1_cv_scores.mean()
            std_f1_cv = f1_cv_scores.std()

            # Almacenar resultados
            cv1_results.append({
                'n_estimators': n_est,
                'max_depth': depth,
                'min_samples_leaf': min_leaf,
                'F1 CV Mean': mean_f1_cv,
                'F1 CV Std': std_f1_cv
            })

            # Actualizar la mejor combinación
            if mean_f1_cv > best_f1_cv:
                best_f1_cv = mean_f1_cv
                best_params = {
                    'n_estimators': n_est,
                    'max_depth': depth,
                    'min_samples_leaf': min_leaf
                }

# Convertir resultados a DataFrame para visualización
cv1_results_df = pd.DataFrame(cv1_results)
print("Resultados de Cross Validation (F1 Score promedio y desviación estándar):")
print(cv1_results_df)

# Imprimir la mejor combinación
print("\nMejor combinación de parámetros:", best_params)
print(f"Mejor F1 Score promedio (Cross Validation): {best_f1_cv:.4f}")

# Entrenar el modelo final con la mejor combinación
best_rf_classifier = RandomForestClassifier(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    min_samples_leaf=best_params['min_samples_leaf'],
    class_weight='balanced',
    random_state=123
)
best_rf_classifier.fit(X_train, y_train)

# Calcular F1 Score en el conjunto de prueba
y_test_pred_rf = best_rf_classifier.predict(X_test)
f1_test = f1_score(y_test, y_test_pred_rf)
print(f"F1 Score en el conjunto de prueba: {f1_test:.4f}")

# Resultados Random Forest
results.append({
    'Modelo': 'Random Forest',
    'F1 Score': f1_score(y_test, y_test_pred_rf),
    'Precisión': precision_score(y_test, y_test_pred_rf),
    'Recall': recall_score(y_test, y_test_pred_rf),
    'AUC-ROC': roc_auc_score(y_test, best_rf_classifier.predict_proba(X_test)[:, 1])
})


"""### Pregunta 1.12

En esta pregunta deberá entrenar _XGBpost Classifier_ realizando un ejercicio previo de _hyperparameter tuning_.

Repita el ejercicio de _Cross Validation_ anterior, pero está vez evaluando:

- `n_estimators = {100, 1000}`
- `max_depth = {5, 50, 100}`
- `learning_rate = {0.05, 0.1, 0.5}`
- `max_leaves= {2, 4, 10}`

Determine la mejor combinación de parámetros y vuelva a entrenar su bosque con esta, reportando el $F_1\ Score$ con la muestra de prueba.

Asegúrese de que sus modelos sean capaces de lidiar con potenciales desbalances. Para esto se recomienda leer la documentación oficial de la *clase* `XGBClassifier()` (parámetros de inicialización): https://xgboost.readthedocs.io/en/latest/python/python_api.html.

"""
pip install xgboost
from xgboost import XGBClassifier

# Calcular scale_pos_weight para manejar el desbalance
n_negative = len(y_train[y_train == 0])  # Número de "no" (0)
n_positive = len(y_train[y_train == 1])  # Número de "yes" (1)
scale_pos_weight = n_negative / n_positive
print(f"scale_pos_weight calculado: {scale_pos_weight:.2f}")

# Definir el espacio de hiperparámetros
param_grid = {
    'n_estimators': [100, 1000],
    'max_depth': [5, 50, 100],
    'learning_rate': [0.05, 0.1, 0.5],
    'max_leaves': [2, 4, 10]
}

# Inicializar listas para almacenar resultados
cv2_results = []
best_f1_cv = -1
best_params = None

# Realizar Cross Validation con K=5 para cada combinación
for n_est in param_grid['n_estimators']:
    for depth in param_grid['max_depth']:
        for lr in param_grid['learning_rate']:
            for leaves in param_grid['max_leaves']:
                # Crear y configurar el modelo
                xgb_classifier = XGBClassifier(
                    n_estimators=n_est,
                    max_depth=depth,
                    learning_rate=lr,
                    max_leaves=leaves,
                    scale_pos_weight=scale_pos_weight,  # Manejo de desbalance
                    random_state=123,
                    eval_metric='logloss'  # Métrica para evitar warnings
                )

                # Calcular F1 Score promedio con validación cruzada (K=5)
                f1_cv_scores = cross_val_score(
                    xgb_classifier, X_train, y_train, cv=5, scoring='f1'
                )
                mean_f1_cv = f1_cv_scores.mean()
                std_f1_cv = f1_cv_scores.std()

                # Almacenar resultados
                cv2_results.append({
                    'n_estimators': n_est,
                    'max_depth': depth,
                    'learning_rate': lr,
                    'max_leaves': leaves,
                    'F1 CV Mean': mean_f1_cv,
                    'F1 CV Std': std_f1_cv
                })

                # Actualizar la mejor combinación
                if mean_f1_cv > best_f1_cv:
                    best_f1_cv = mean_f1_cv
                    best_params = {
                        'n_estimators': n_est,
                        'max_depth': depth,
                        'learning_rate': lr,
                        'max_leaves': leaves
                    }

# Convertir resultados a DataFrame para visualización
cv2_results_df = pd.DataFrame(cv2_results)
print("Resultados de Cross Validation (F1 Score promedio y desviación estándar):")
print(cv2_results_df)

# Imprimir la mejor combinación
print("\nMejor combinación de parámetros:", best_params)
print(f"Mejor F1 Score promedio (Cross Validation): {best_f1_cv:.4f}")

# Entrenar el modelo final con la mejor combinación
best_xgb_classifier = XGBClassifier(
    n_estimators=best_params['n_estimators'],
    max_depth=best_params['max_depth'],
    learning_rate=best_params['learning_rate'],
    max_leaves=best_params['max_leaves'],
    scale_pos_weight=scale_pos_weight,
    random_state=123,
    eval_metric='logloss'
)
best_xgb_classifier.fit(X_train, y_train)

# Calcular F1 Score en el conjunto de prueba
y_test_pred_xgb = best_xgb_classifier.predict(X_test)
f1_test = f1_score(y_test, y_test_pred)
print(f"F1 Score en el conjunto de prueba: {f1_test:.4f}")

# Resultados XGBoost
results.append({
    'Modelo': 'XGBoost',
    'F1 Score': f1_score(y_test, y_test_pred_xgb),
    'Precisión': precision_score(y_test, y_test_pred_xgb),
    'Recall': recall_score(y_test, y_test_pred_xgb),
    'AUC-ROC': roc_auc_score(y_test, best_xgb_classifier.predict_proba(X_test)[:, 1])
})


"""### Pregunta 1.13

Entrene un modelo _Naïve Bayes_. Asegúrese de que el modelo sea capaz de lidiar con potenciales desbalances. Para esto se recomienda leer la documentación oficial de la *clase* `GaussianNB()` (parámetros de inicialización): https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html.


Despliege el $F_1\ Score$ del modelo sobre la base de entrenamiento.
"""

from sklearn.naive_bayes import GaussianNB

# Crear el modelo GaussianNB con priors ajustados para manejar desbalance
nb_classifier = GaussianNB(priors=[0.5, 0.5])

# Entrenar el modelo con el conjunto de entrenamiento
nb_classifier.fit(X_train, y_train)

# Predecir sobre el conjunto de prueba
y_test_pred_nb = nb_classifier.predict(X_test)

# Calcular el F1 Score en el conjunto de prueba
f1_test = f1_score(y_test, y_test_pred_nb)
print(f"F1 Score en el conjunto de prueba: {f1_test:.4f}")

# Resultados Naïve Bayes (GaussianNB)
results.append({
    'Modelo': 'Naïve Bayes',
    'F1 Score': f1_score(y_test, y_test_pred_nb),
    'Precisión': precision_score(y_test, y_test_pred_nb),
    'Recall': recall_score(y_test, y_test_pred_nb),
    'AUC-ROC': roc_auc_score(y_test, nb_classifier.predict_proba(X_test)[:, 1])
})

"""---

## Análisis Comparativo de Modelos de Clasificación

### Pregunta 1.14

Evalúe y compare el desempeño de los modelos teniendo en cuenta la naturaleza del conjunto de datos. ¿Cómo afectó el desbalance a cada modelo? ¿Qué modelo manejó mejor el balance o desbalance de clases y cuáles son sus conclusiones generales sobre su rendimiento comparado?

Para que la comparación sea clara, es útil indicar qué métricas de evaluación utilizo para comparar los modelos (por ejemplo, F1-score, precisión, recall, AUC, etc.).
"""

# Mostrar resultados
finalresults_df = pd.DataFrame(results)

# Mostrar todas las columnas
pd.set_option('display.max_columns', None)

print("Comparación de modelos (en conjunto de prueba):")
print(finalresults_df)

"""---


Comparando el rendimiento de los modelos, Random Forest y XGBoost son los mejores en desempeño general, presentando un F1 Score de 0.4204 y 0.4059, respectivamente), superando significativamente a los Árboles de Decisión y Naïve Bayes. En cuanto a los Árboles de Decisión, la regularización que impone un máximo de profundidad de 8 mejora de manera notable el desempeño relativo al modelo sin regularización.

Respecto al desbalance, éste afecta más a los modelos sin ajustes específicos (árboles de decisión), reduciendo su capacidad para identificar la clase minoritaria. Por otro lado, los ajustes como “class weight”, “scale pos weight” o “priors” permiten mejorar el recall, pero a costa de la precisión.


---

# KMeans & PCA

La guerra comercial desatada levanta un montón de preguntas de interés en economía. Una de estas es: ¿Qué países sufrirán de forma similar las consecuencias de esta? ¿Quiénes serían ganadores? ¿Quienes serían perdedores?

En esta parte exploraremos el aporte que los métodos de Clustering de Machine Learning pueden hacer a la respuesta de esta cuestión. Nos enfocaremos en variables relacionadas al comercio exterior para determinar similitud entre grupos de países. Más específicamente, en variables relacionadas a las exportaciones.

Para esto, considere las siguientes variables que describen el perfil exportador de cada país:

- GDP growth (annual %)
- Agricultural raw materials exports (% of merchandise exports)
- Computer, communications and other services (% of commercial service exports)
- Exports of goods and services (% of GDP)
- Food exports (% of merchandise exports)
- Fuel exports (% of merchandise exports)
- High-technology exports (% of manufactured exports)
- ICT service exports (% of service exports, BoP)
- Insurance and financial services (% of service exports, BoP)
- International tourism, receipts (% of total exports)
- Manufactures exports (% of merchandise exports)
- Merchandise exports to high-income economies (% of total merchandise exports)
- Merchandise exports to low- and middle-income economies in Europe & Central Asia (% of total merchandise exports)
- Ores and metals exports (% of merchandise exports)
- Taxes on exports (% of tax revenue)
- Transport services (% of commercial service exports)


La idea será determinar similitud entre países y las variables qué son más relevantes a la hora de determinarla.

Debido a que 2023 es el año reciente con una mayor riqueza de datos, y debido a que nos interesan cambios respecto al _status quo_ en el contexto actual, trabajaremos con datos sólo de aquel año.

Database Source: World bank & Trade Analysis Information System.

### Pregunta 2.0

Cargue la base de datos disponible en el siguiente link: https://raw.githubusercontent.com/olherreragz/EAE3709-2025-1/refs/heads/main/T2_data/Data.cs.

En el siguiente link podrá encontrar información adicional sobre el _dataset_ por si lo llega a necesitar: https://github.com/olherreragz/EAE3709-2025-1/blob/main/T2_data/Series_Metadata.csv.

Elimine las columnas `"Series Code"` y `"Country Code"`. Luego, transforme su _dataframe_ de formato _Long_ a _Wide_. Su _dataframe_ final deberá contener una columna que indique el nombre del país y columnas que indiquen la medida de exportación correspondiente:

`<Country>, <SeriesName1>, <SeriesName2> ....`



![Long to Wide Format](https://tavareshugo.github.io/r-intro-tidyverse-gapminder/fig/07-data_shapes.png)
"""



"""### Pregunta 2.1

Debido a que la pregunta de interés es sobre identificación de grupos de países, elime filas donde `Country Name` sea igual a cualquiera de los siguientes valores:

- "Africa Eastern and Southern"  
- "Africa Western and Central"  
- "Arab World"  
- "Caribbean small states"  
- "Central Europe and the Baltics"  
- "Early-demographic dividend"  
- "East Asia & Pacific"  
- "East Asia & Pacific (IDA & IBRD countries)"  
- "East Asia & Pacific (excluding high income)"  
- "Euro area"  
- "Europe & Central Asia"  
- "Europe & Central Asia (IDA & IBRD countries)"  
- "Europe & Central Asia (excluding high income)"  
- "European Union"  
- "Fragile and conflict affected situations"  
- "Heavily indebted poor countries (HIPC)"  
- "High income"  
- "IBRD only"  
- "IDA & IBRD total"  
- "IDA blend"  
- "IDA only"  
- "IDA total"  
- "Late-demographic dividend"  
- "Latin America & Caribbean"  
- "Latin America & Caribbean (excluding high income)"  
- "Latin America & the Caribbean (IDA & IBRD countries)"  
- "Least developed countries: UN classification"  
- "Low & middle income"  
- "Low income"  
- "Lower middle income"  
- "Middle East & North Africa"  
- "Middle East & North Africa (IDA & IBRD countries)"  
- "Middle East & North Africa (excluding high income)"  
- "Middle income"  
- "North America"  
- "Not classified"  
- "OECD members"  
- "Other small states"  
- "Pacific island small states"  
- "Post-demographic dividend"  
- "Pre-demographic dividend"  
- "Small states"  
- "South Asia"  
- "South Asia (IDA & IBRD)"  
- "Sub-Saharan Africa"  
- "Sub-Saharan Africa (IDA & IBRD countries)"  
- "Sub-Saharan Africa (excluding high income)"  
- "Upper middle income"  
- "World"



"""



"""### Pregunta 2.2

Reemplace todos los valores `".."` por `NA`. Luego, transforme las columnas a variables numéricas y despliegue estadísticas descriptivas.

"""



"""### Pregunta 2.3

Grafique la distribución empírica de todas las variables numéricas del dataset. Comente sobre cada una de ellas.
"""



"""---


*Escriba* su respuesta en esta celda...


---

## Missing Values

### Pregunta 2.4

Realice análisis de valores vacíos y tratamiento de estos. Se premiará mantener el mayor número de observaciones posibles bajo criterios razonables. Luego de esta pregunta no deben quedar nulos en el _dataframe_ a utilizar.
"""



"""## Outliers

### Pregunta 2.5

Realice análisis de datos _outliers_. De ser necesario aplique un procesamiento de outliers, de no ser necesario justifique.

Se premiará el buen criterio fundamentado.
"""



"""---


*Escriba* su respuesta en esta celda...


---

## Feature Engineering

### Pregunta 2.6

Realice _feature engineering_ de las variables numéricas (para luego entrenar un modelo _K-Means_ con estas). Realice los ajustes que estime conveniente, pero su tratamiento debe incluir por lo menos estandarización de las variables.

Más adelante se le pedirá comentar sobre los resultados de sus modelos, por lo que considere que sus decisiones podrían condicionar futuras preguntas.
"""



"""## K-Means

### Pregunta 2.7

Considerando el dataframe completo (todas sus columnas atributos), determine el número óptimo de clusters mediante el "Elbow method". Considere un rango $k \in [1, 30]$. Describa el comportamiento observado de la curva.
"""



"""---


*Escriba* su respuesta en esta celda...


---

### Pregunta 2.8

Un problema habitual del modelo _K-Means_ es que pierde desempeño a mayor dimensionalidad.

En esta pregunta defina **por lo menos 5 combinaciones de columnas** (mínimo 2 columnas y máximo 4). Para cada combinación determine el número óptimo de clusters mediante el "Elbow method" considerando $k \in [1, 30]$.

Un posible enfoque para lo anterior es tomar un _approach_ estadístico y escoger variables que empíricamente muestran mayores indicios de poder diferenciador entre las observaciones. De todas formas, recuerde que la pregunta de interés es "¿qué países se verán expuestos de forma similar a la guerra comercial?". En este sentido, es altamente recomendable escoger variables con una mayor relación al fenómeno de la guerra comercial, ya que puede ser útil para discernir de qué forma podría verse expuesto cada grupo.
"""



"""---


*Escriba* su respuesta en esta celda...


---

### Pregunta 2.9

Escoja la combinación de columnas que considera adecuada emplear, refiriéndose tanto al comportamiento observado de la curva, como al nivel de la métrica empleada al que se alcanza la convergencia.

Estime el modelo con $k$ óptimo y las columnas escogidas. Visualice los grupos graficando _scatterplots_. Considere gráficos en 3 dimensiones si su modelo posee 3 o más variables.

Describa en qué se diferencian los grupos encontrados y de qué manera le podría afectar la guerra comercial a cada uno. Liste una pequeña muestra representativa de los países que contiene cada grupo.
"""



"""---


*Escriba* su respuesta en esta celda...


---

## PCA

_PCA_ es una técnica de reducción de dimensionalidad que nos permite trabajar sobre un set de componentes principales ortogonales (mayor facilidad para los modelos a la hora de detectar patrones), además de permitirnos visualizar espacios multidimensionales, quedándonos sobre el espacio de PCA.

### Pregunta 2.10

Utilizando todas las variables originales y $k$ óptimo escogido en la pregunta anterior, estime un PCA con 2 componentes principales. Luego, estime un modelo _K-Means_ sobre los componentes principales obtenidos. Grafique las observaciones en un _scatterplot_ de 2 dimensiones. Coloree los puntos usando los label de los clusters obtenidos.

¿Cuáles son las principales diferencias entre los grupos? Encuentre las variables más definitivas a la hora de agrupar países con los 2 primeros componentes principales.
"""



"""---


*Escriba* su respuesta en esta celda...


---

### Pregunta 2.11


Repita lo anterior  (descripción incluida), pero con 3 componentes principales y un _scatterplot_ en 3 dimensiones.
"""



"""---


*Escriba* su respuesta en esta celda...


---

### Pregunta 2.12

Otro de los usos de los más comúnes de _PCA_, además de _feature engineering_, es la visualización. En esta sección utilizaremos _PCA_ para confirmar visualmente que estamos clusterizando países similares.

Usando el diccionario en la celda de abajo agrega una columna de continente al _dataframe_ de trabajo.
"""

country_continent_mapping = {
    'Albania': 'Europe',
    'Argentina': 'South America',
    'Armenia': 'Asia',
    'Aruba': 'North America', # Usually classified with the Caribbean
    'Australia': 'Oceania',
    'Austria': 'Europe',
    'Azerbaijan': 'Asia',
    'Bahrain': 'Asia',
    'Belgium': 'Europe',
    'Belize': 'North America',
    'Bolivia': 'South America',
    'Bosnia and Herzegovina': 'Europe',
    'Botswana': 'Africa',
    'Brazil': 'South America',
    'Brunei Darussalam': 'Asia',
    'Bulgaria': 'Europe',
    'Burkina Faso': 'Africa',
    'Cabo Verde': 'Africa',
    'Canada': 'North America',
    'Chile': 'South America',
    'China': 'Asia',
    'Colombia': 'South America',
    'Congo Dem. Rep.': 'Africa',
    'Costa Rica': 'North America',
    'Croatia': 'Europe',
    'Cyprus': 'Asia',
    'Czechia': 'Europe',
    'Denmark': 'Europe',
    'Dominican Republic': 'North America',
    'Ecuador': 'South America',
    'Egypt': 'Africa', # Transcontinental, but usually Africa
    'El Salvador': 'North America',
    'Estonia': 'Europe',
    'Ethiopia': 'Africa',
    'Finland': 'Europe',
    'France': 'Europe',
    'Gambia': 'Africa',
    'Georgia': 'Asia',
    'Germany': 'Europe',
    'Ghana': 'Africa',
    'Greece': 'Europe',
    'Guatemala': 'North America',
    'Honduras': 'North America',
    'Hong Kong': 'Asia',
    'Hungary': 'Europe',
    'Iceland': 'Europe',
    'India': 'Asia',
    'Indonesia': 'Asia',
    'Ireland': 'Europe',
    'Italy': 'Europe',
    'Japan': 'Asia',
    'Jordan': 'Asia',
    'Kazakhstan': 'Asia', # Transcontinental, mostly Asia
    'Kenya': 'Africa',
    'Korea Rep.': 'Asia',
    'Latvia': 'Europe',
    'Lesotho': 'Africa',
    'Lithuania': 'Europe',
    'Luxembourg': 'Europe',
    'Macao': 'Asia',
    'Malaysia': 'Asia',
    'Malta': 'Europe',
    'Mauritania': 'Africa',
    'Mauritius': 'Africa',
    'Mexico': 'North America',
    'Moldova': 'Europe',
    'Montenegro': 'Europe',
    'Morocco': 'Africa',
    'Mozambique': 'Africa',
    'Namibia': 'Africa',
    'Netherlands': 'Europe',
    'New Zealand': 'Oceania',
    'Nicaragua': 'North America',
    'Niger': 'Africa',
    'North Macedonia': 'Europe',
    'Norway': 'Europe',
    'Pakistan': 'Asia',
    'Panama': 'North America',
    'Paraguay': 'South America',
    'Peru': 'South America',
    'Philippines': 'Asia',
    'Poland': 'Europe',
    'Portugal': 'Europe',
    'Romania': 'Europe',
    'Samoa': 'Oceania',
    'Saudi Arabia': 'Asia',
    'Senegal': 'Africa',
    'Singapore': 'Asia',
    'Slovak Republic': 'Europe',
    'Slovenia': 'Europe',
    'South Africa': 'Africa',
    'Spain': 'Europe',
    'Sri Lanka': 'Asia',
    'Sweden': 'Europe',
    'Switzerland': 'Europe',
    'Tanzania': 'Africa',
    'Thailand': 'Asia',
    'Timor-Leste': 'Asia',
    'Tunisia': 'Africa',
    'Turkiye': 'Asia', # Transcontinental, mostly Asia
    'Uganda': 'Africa',
    'Ukraine': 'Europe',
    'United Kingdom': 'Europe',
    'United States': 'North America',
    'Uruguay': 'South America',
    'Uzbekistan': 'Asia',
    'Zambia': 'Africa'
}

"""### Pregunta 2.13

Realice un PCA de 2 componentes principales, grafique un _scatterplot_ del resultado y coloree cada punto según su continente. ¿Que se obserba?

"""



"""---


*Escriba* su respuesta en esta celda...


---

### Pregunta 2.14

Genera el mismo gráfico anterior, pero ahora coloree los puntos del scatterplot usando los clusters de un KMeans de 3 clústers y `random_state=42`. Describa los clusters según el gráfico.
"""



"""---


*Escriba* su respuesta en esta celda...


---

### Pregunta 2.15

¿Cual es la proporcion de cada cluster por continente?
"""



"""### Pregunta 2.16

¿Cuál es la característica numérica más alta en magnitud por cluster (usando sus centroides)? ¿Cuál es su signo? ¿Como nos ayuda a interpretar los clusters? ¿Que podemos concluir de cada uno de estos clusters en contexto de la guerra comercial?

"""



"""---


*Escriba* su respuesta en esta celda...


---
"""